jupyterhub:
  auth:
    type: dummy
    dummy:
      password: 
  
  scheduling:
    userScheduler:
      enabled: true
    userPods:
      nodeAffinity:
        matchNodePurpose: require
    podPriority:
      enabled: true
      globalDefault: false
      defaultPriority: 0
      userPlaceholderPriority: -10
    corePods:
      nodeAffinity:
        matchNodePurpose: require
    userPlaceholder:
      enabled: true
      replicas: 1

  cull:
    enabled: true
    timeout: 3600    # cull inactive servers after this long
    every: 600       # how often to check 
    maxAge: 28800        # cull servers this old, even if active (0 disables)

  proxy:
    # this should get replaced when deployed with scripts/deploy_from_settings.sh
    secretToken: "78767972dd3d0db5a98fb56554f4484daa57bc35bd73dc61aafebed8ac9fe3df"
    service: 
      type: ClusterIP
    https:
      enabled: true
      type: offload
  singleuser:
    # if nodes are labeled with their cost in cents-per-hour, this should help
    # schedule pods on the cheapest node they can fit on (notwithstanding influences by the custom scheduler and various requirements)
    extraNodeAffinity:
      preferred:
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "10"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "20"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "30"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "40"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "50"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "60"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "70"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "80"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "90"
      - weight: 100
        preference: 
          matchExpressions:
          - key: cents-per-hour
            operator: Lt
            values:
            - "100"
      # looks like these should be set null to delete the key (including those defaulted in the jupyterhub chart) for the c.Spawner limits below to be used
    memory:
      limit: 1.0G
      guarantee: 0.5G
    cpu:
      limit: 1.0
      guarantee: 0.1
    image:
      name: oneilsh/ktesting-datascience-notebook
      tag: "v1.1.6"
    defaultUrl: "/lab/tree/{username}"
    cmd: ["start-singleuser.sh", "--allow-root"]  # don't think I need to add --allow-root... 
    storage:
      type: none
      extraVolumes:
        - name: data-volume
          configMap:
            name: data-configmap
        - name: start-volume
          configMap:
            name: start-configmap
            defaultMode: 0554
      extraVolumeMounts:
        - name: start-volume
          mountPath: /usr/local/bin/start.sh
          subPath: start.sh
        - name: data-volume
          mountPath: /usr/local/bin/data

    extraEnv:
      NFS_SVC_HOME: ""
    uid: 0
    fsgid: 0
    startTimeout: 1200
    profileQuota:
      checkEvery: 10  # default 600
      quotaDbFilename: "/srv/jupyterhub/profile_quotas.sqlite" # default "/srv/jupyterhub/profile_quotas.sqlite"
    profileList:
    - display_name: "Standard Profile"
      slug: "standard"
      description: "The basics"
      default: True
    - display_name: "Fancy"
      description: "Oh you fancy huh"
      slug: "fancy"
      kubespawner_override:
        cpu_gaurantee: 0.4
        cpu_limit: 1.2
        mem_guarantee: "0.3G"
        mem_limit: "0.7G"
      disabled: true # set to true to disable this profile altogether
      quota:
        costTokensPerHour: 0.1  # default 1
        minBalanceToSpawn: 0.05 # default 0.0
        admins:
          active: true   # default true
          initialBalance: 5   # default float("inf")
          newTokensPerDay: 0.4 # default 0.0
          maxBalance: 10  # default float("Inf")
        users:
          active: true
          initialBalance: 5
          newTokensPerDay: 0.4
          maxBalance: 10
    - display_name: "Fancy 2"
      description: "Oh you fancy huh"
      slug: "fancy2"
      kubespawner_override:
        cpu_gaurantee: 0.2
        cpu_limit: 1.1
        mem_guarantee: "0.4G"
        mem_limit: "0.8G"
      quota:
        costTokensPerHour: 360
        minBalanceToSpawn: 5
        admins:
          active: true
          initialBalance: 60
          newTokensPerDay: 60
          maxBalance: 10
        users:
          active: true
          initialBalance: 5
          newTokensPerDay: 0.4
          maxBalance: 10

  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: "nginx"
      nginx.org/mergeable-ingress-type: "minion"
      nginx.org/websocket-services: "proxy-public"
    hosts:
    - dummy.value.org 
    tls:
    - hosts:
      - dummy.value.org

  hub:
    baseUrl: /
    db:
      type: "sqlite-pvc"
      upgrade: false 
    image:
      name: oneilsh/ktesting-k8s-hub
      tag: "v1.3.2" 
    uid: 0
    fsgid: 0
    extraVolumes:
      - name: data-volume
        configMap:
          name: data-configmap
      - name: start-hub-volume
        configMap:
          name: start-hub-configmap
          defaultMode: 0554
    extraVolumeMounts:
      - name: data-volume
        mountPath: /usr/local/bin/data
      - name: start-hub-volume
        mountPath: /usr/local/bin/start-hub.sh
        subPath: start-hub.sh

    extraConfig:
      concurrentSpawn:
        c.JupyterHub.concurrent_spawn_limit = 400
      templatePath:
        c.JupyterHub.template_paths = ['/nfs_home/.hub_local/announcement_templates']
      basedir: |
        c.KubeSpawner.notebook_dir = "/home"

      nfspriv: |
        from kubernetes import client
        def modify_pod_hook(spawner, pod):
            pod.spec.containers[0].security_context = client.V1SecurityContext(
                privileged=True,
                capabilities=client.V1Capabilities(
                    add=['SYS_ADMIN']
                )
            )
            return pod
        c.KubeSpawner.modify_pod_hook = modify_pod_hook

      profiles: |
        from jhprofilequota import profile_db

        quota_db_filename = get_config('singleuser.profileQuota.quotaDbFilename', '/srv/jupyterhub/profile_quotas.sqlite')
        profile_db.create_db(quota_db_filename) 
       
        checker_cmd = ['python3','-m', 'jhprofilequota']
        base_url = c.JupyterHub.get('base_url', '/')
        checker_cmd.append( '--url=http://127.0.0.1:8081' + url_path_join(base_url, 'hub/api'))
    
        checker_cmd.append('--quota_db_filename=%s' % quota_db_filename)
        
        check_every = get_config('singleuser.profileQuota.checkEvery', 600)
        checker_cmd.append('--check_every=%s' % check_every)

        profiles_json = json.dumps(get_config('singleuser.profileList', "[]"))
        # this feels awkward... but I don't want to make the profiles 'state' for the service if I can help it
        checker_cmd.append("--profiles_json=" + profiles_json)
    
        c.JupyterHub.services.append({
          'name': 'quota_checker',
          'admin': True,
          'command': checker_cmd
        })

        c.KubeSpawner.profile_form_template = """ 
        <script>
        // JupyterHub 0.8 applied form-control indisciminately to all form elements.
        // Can be removed once we stop supporting JupyterHub 0.8
        $(document).ready(function() {
            $('#kubespawner-profiles-list input[type="radio"]').removeClass('form-control');
        });
        </script>
        <style>
        /* The profile description should not be bold, even though it is inside the <label> tag */
        #kubespawner-profiles-list label p {
            font-weight: normal;
        }
        </style>
        <div class='form-group' id='kubespawner-profiles-list'>
        {% for profile in profile_list %}
        <label for='profile-item-{{ profile.slug }}' class='form-control input-group'>
            <div class='col-md-1'>
                <input type='radio' name='profile' id='profile-item-{{ profile.slug }}' value='{{ profile.index }}' {% if profile.default %}checked{% endif %} /> {% if profile["quotaDisplay"]["disabled"] %}disabled{% endif %}/>
            </div>
            <div class='col-md-11'>
                <strong>{{ profile.display_name }}</strong>
                {% if profile.description %}
                <p>{{ profile.description }}</p>
                {% endif %}
                {% if profile.hasQuota %}
                  <p>Quota Balance: {{ round(profile["quotaDisplay"]["balanceTokens"], 1) }} Hours (Add Rate: {{ profile["quotaDisplay"]["rateHoursPerDay"] }} Hours/Day, Max Balance: {{ profile["quotaDisplay"]["maxBalance"] }} Hours, Min Required Start: {{ profile["quotaDisplay"]["minToStart"] }} Hours) <a href="">What's This?</a></p> 
                {% endif %}
            </div>
        </label>
        {% endfor %}
        </div>
        """

        from kubespawner import KubeSpawner
        class KubeSpawnerWithProfileInfo(KubeSpawner):
            def get_state(self):
                state = super().get_state()
                state['profile_slug'] = self.user_options["slug"]
                return state

        c.JupyterHub.spawner_class = KubeSpawnerWithProfileInfo

        def profiles_gen(spawner):
            profiles_list = get_config("singleuser.profileList", [])

            conn = profile_db.get_connection(quota_db_filename)

            profile_db.update_user_tokens(conn, profiles_list, spawner.user.name, spawner.user.admin)
            updated_profiles = profile_db.get_profiles_by_balance(conn, profiles_list, spawner.user.name, spawner.user.admin)

            profile_db.close_connection(conn)
            return updated_profiles 

        c.KubeSpawner.profile_list = profiles_gen

      authsetting: |
        import os
        import json

        env_auth_type = os.environ["AUTH_TYPE"]

        if env_auth_type == "native":
          from nativeauthenticator import NativeAuthenticator
          from tornado import gen

          class NativeExtraAuthenticator(NativeAuthenticator):
            @gen.coroutine
            def pre_spawn_start(self, user, spawner):
              # for use by scripts in start-notebook.d
              # NB_USER is the username logged in with
              # ADMIN_USER is "True" if they're an admin, "False" otherwise
              spawner.environment['NB_USER'] = user.name
              spawner.environment['ADMIN_USER'] = str(user.admin)
              # below for debugging
              #spawner.environment['NB_UID'] = "1000"
              #spawner.environment['GRANT_SUDO'] = "yes"
       
          c.JupyterHub.authenticator_class = NativeExtraAuthenticator
          # location in docker image
          c.JupyterHub.template_paths = ["/home/jovyan/nativeauthenticator/nativeauthenticator/templates/"]       

        elif env_auth_type == "lti":
          from ltiauthenticator import LTIAuthenticator
          from tornado import gen

          class LTIExtraAuthenticator(LTIAuthenticator):
            @gen.coroutine
            def pre_spawn_start(self, user, spawner):
              # for use by scripts in start-notebook.d
              # NB_USER is the username logged in with
              # ADMIN_USER is "True" if they're an admin, "False" otherwise
              spawner.environment['NB_USER'] = user.name
              spawner.environment['ADMIN_USER'] = str(user.admin)
              # below for debugging
              #spawner.environment['NB_UID'] = "1000"
              #spawner.environment['GRANT_SUDO'] = "yes"
       
          c.JupyterHub.authenticator_class = LTIExtraAuthenticator
          c.LTIExtraAuthenticator.consumers = {
            os.environ['LTI_CLIENT_KEY']: os.environ['LTI_CLIENT_SECRET']
          } 

          # take JSON-encoded environment vars 
          c.LTIExtraAuthenticator.user_id_keys = json.loads(os.environ["LTI_ID_KEYS"]) #eg '["custom_canvas_user_login_id", "lis_person_contact_email_primary", "custom_canvas_user_login_id"]'
          c.LTIExtraAuthenticator.user_id_regexes = json.loads(os.environ["LTI_ID_REGEXES"]) #eg '["(^[^@]+)@oregonstate.edu$", "(^[^@]+@[^@]+$)", "(^[0-9a-f]{6,6})[0-9a-f]*$"]'
          c.LTIExtraAuthenticator.user_admin_roles = json.loads(os.environ["LTI_ADMIN_ROLES"]) #eg '["Instructor", "TeachingAssistant", "ContentDeveloper"]'

        else:
          from jupyterhub.auth import DummyAuthenticator
          from tornado import gen

          class DummyExtraAuthenticator(DummyAuthenticator):
            @gen.coroutine
            def pre_spawn_start(self, user, spawner):
              spawner.environment['NB_USER'] = user.name
              spawner.environment['ADMIN_USER'] = str(user.admin)
       
          ## read admin user list from provided values 
          c.JupyterHub.authenticator_class = DummyExtraAuthenticator

        # don't use the ADMIN_USERS list if using lti auth - admin info is picked up in container
        if env_auth_type != "lti": 
          admin_users_comma_sep = os.environ["ADMIN_USERS"]  # grab admin usernames from environment variable set in questions.yaml
          admin_users = set([entry.strip() for entry in admin_users_comma_sep.split(",")]) # split it into a set to use
          c.Authenticator.admin_users = admin_users

        # allow admins to access user servers via UI (needs fixing, may not work with LTI?)
        c.JupyterHub.admin_access = True

