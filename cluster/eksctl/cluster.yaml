# for eksctl, managed nodegroups
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: eksdevc
  region: us-west-2

# default availabilityzones unless otherwise specified?
availabilityZones: ["us-west-2a", "us-west-2b", "us-west-2c"]

# eks now has managedNodeGroups, but they don't support taints, which are required for jupyterhub to put user pods in the right nodegroup. 
# one could use 'kubectl taint' to add taints after creation, but new nodes created by an autoscaler won't have it
# a potential workaround: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/299#issuecomment-413924333
# but since this requires modifying the userdata, it can't be used with managedNodeGroups, and so we can just use taints: in the nodeGroup definition (which was added after that comment: https://github.com/weaveworks/eksctl/pull/703)
# ALSO, as CJ found, nodegroups with EBS volumes cannot span availability zones, since EBS vols dont, effectively making HA incompatible with EBS: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/299#issuecomment-529029316
# hence the jhcontrolplane nodegroup, which does use ebs volumes (in the current config) is restricted to just one availability zone
# doing HA will require another storage solution (EFS?) or another cloud provider (GKE has so many fewer problems...)
nodeGroups:
  # I'm not sure which nodegroup eksctl will use for the kubernetes control plane - 
  # perhaps it's spread across them?
  # this jhcontrolplane will be used for jupyterhub core components (https://zero-to-jupyterhub.readthedocs.io/en/latest/administrator/optimization.html#using-a-dedicated-node-pool-for-users)
  # the other, workers, is for jh user pods
  # t3.large is EBS optimized - note that EACH NODE CAN SUPPORT A MAX OF 28 EBS ATTACHMENTS (actually 27; https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/volume_limits.html)
  - name: jhcontrolplane
    instanceType: t3.large
    availabilityZones: ["us-west-2b"]
    iam: 
      withAddonPolicies:
        autoScaler: true
    minSize: 2
    maxSize: 2
    desiredCapacity: 2
    volumeSize: 20
    # to allow ssh into the nodes
    #ssh:
    #  allow: true
    #  publicKeyPath: ~/.ssh/ec2_id_rsa.pub
    #  # new feature for restricting SSH access to certain AWS security group IDs
    #  sourceSecurityGroupIds: ["sg-00241fbb12c607007"]
    labels: 
      role: jhcontrolplane
      # the "volatile" flag indicates whether nodes may come and go frequently - for the control plane, this is false
      volatile: "false"
      hub.jupyter.org/node-purpose: core
    tags:
      nodegroup-role: jhcontrolplane
    preBootstrapCommands:
        # enables docker bridge network so binderhub stuff works
        # see https://gist.github.com/tobemedia/2144d74d232ccce8972613e8ae13b054
        # and https://discourse.jupyter.org/t/binder-deployed-in-aws-eks-domain-name-resolution-errors/766
        # Replicate what --enable-docker-bridge does in /etc/eks/bootstrap.sh
        # Enabling the docker bridge network. We have to disable live-restore as it
        # prevents docker from recreating the default bridge network on restart
        # modprope nfs and nfsd lets hosts (and containers) serve nfs
       - "cp /etc/docker/daemon.json /etc/docker/daemon_backup.json"
       - "echo -e '.bridge=\"docker0\" | .\"live-restore\"=false' >  /etc/docker/jq_script"
       - "jq -f /etc/docker/jq_script /etc/docker/daemon_backup.json | tee /etc/docker/daemon.json"
       - "systemctl restart docker"
       - "modprobe nfs"
       - "modprobe nfsd"

  - name: workers
    instanceType: t2.medium
    iam: 
      withAddonPolicies:
        autoScaler: true
    minSize: 1
    maxSize: 4
    desiredCapacity: 2
    volumeSize: 20
    ###availabilityZones: ["us-west-2a", "us-west-2b", "us-west-2c"]
    labels: 
      role: workers
      volatile: "true"
      hub.jupyter.org/node-purpose: user
    taints:
      hub.jupyter.org/dedicated: user:NoSchedule
    tags:
      nodegroup-role: workers
    preBootstrapCommands:
        # enables docker bridge network so binderhub stuff works
        # see https://gist.github.com/tobemedia/2144d74d232ccce8972613e8ae13b054
        # and https://discourse.jupyter.org/t/binder-deployed-in-aws-eks-domain-name-resolution-errors/766
        # Replicate what --enable-docker-bridge does in /etc/eks/bootstrap.sh
        # Enabling the docker bridge network. We have to disable live-restore as it
        # prevents docker from recreating the default bridge network on restart
        # modprope nfs and nfsd lets hosts (and containers) serve nfs
       - "cp /etc/docker/daemon.json /etc/docker/daemon_backup.json"
       - "echo -e '.bridge=\"docker0\" | .\"live-restore\"=false' >  /etc/docker/jq_script"
       - "jq -f /etc/docker/jq_script /etc/docker/daemon_backup.json | tee /etc/docker/daemon.json"
       - "systemctl restart docker"
       - "modprobe nfs"
       - "modprobe nfsd"

